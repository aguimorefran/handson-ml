{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\n",
    "\n",
    "We could use voting and get better results, using the prediction that get the most votes. The law of large numbers explains that when a probability is low, if repeated enough, the average results of the experiment approach the ideal probability. This can be achieved easily with almost random guessers.\n",
    "\n",
    "## 2. What is the difference between hard and soft voting classifiers?\n",
    "\n",
    "Soft voting gives more weight to the most confident values (highest proba). Hard voting uses the mode prediction.\n",
    "\n",
    "## 3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?\n",
    "\n",
    "Bagging can be sped up across multiple server because they are independent of each other. \n",
    "Pasting is the same but without replacement.\n",
    "In boosting each predictor depends on the results of the previous predictor, so it can not be parallelized.\n",
    "Stacking uses various predictos and then blends them. These different predictors are independant of each other so they could be parallelized too.\n",
    "Same with Random Forests.\n",
    "\n",
    "## 4. What is the benefit of out-of-bag evaluation?\n",
    "\n",
    "It serves as a test set because those samples have not been yet seen by the predictors.\n",
    "\n",
    "## 5. What makes Extra-Trees more random than regular random forests? How can this extra randomness help? Are extra trees slower or faster than regular random forests?\n",
    "\n",
    "By randomizing the thresholds of each feature instead of finding the best ones.\n",
    "This randomness ensures regularization. It speeds up the process because finding the best threshold is the most time consuming task when training Trees.\n",
    "\n",
    "## 6. If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how?\n",
    "\n",
    "We could increase the learning rate, increase the number of estimators, or use a more regularizing base estimator.\n",
    "\n",
    "## 7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\n",
    "\n",
    "As with any other predictor, we should decrease the learning rate."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
