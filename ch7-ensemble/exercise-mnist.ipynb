{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load the MNIST data (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g., use the first 40,000 instances for training, the next 10,000 for validation, and the last 10,000 for testing). Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False)\n",
    "\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "plot_digit(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (10000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=10000)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=10000)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest, Extra tree and SVM classifier for mnist dataset\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "# Halving\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest:  {'max_depth': 300, 'n_estimators': 300}\n",
      "Best score for Random Forest:  0.9630000000000001\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "param_grid_rf = {'n_estimators': [100, 200, 300], 'max_depth': [100, 200, 300]}\n",
    "rf = RandomForestClassifier()\n",
    "rf_halving = HalvingGridSearchCV(rf, param_grid_rf, cv=2, scoring='accuracy', n_jobs=-1, factor=2)\n",
    "rf_halving.fit(X_train, y_train)\n",
    "print('Best parameters for Random Forest: ', rf_halving.best_params_)\n",
    "print('Best score for Random Forest: ', rf_halving.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Random Forest:  0.9704\n",
      "F1 weighted score for Random Forest:  0.9703667430931813\n",
      "Confusion matrix for Random Forest: \n",
      " [[ 940    0    1    0    0    2    1    0    4    1]\n",
      " [   0 1093    7    0    3    0    2    4    1    1]\n",
      " [   5    1 1027    3    4    0    3    5    3    0]\n",
      " [   1    3    7  985    1    7    1    3    7    6]\n",
      " [   3    2    0    0  937    0    0    2    4   11]\n",
      " [   4    0    1   10    2  819    8    1    6    3]\n",
      " [   7    3    2    0    0    9  963    0    0    0]\n",
      " [   0    4    9    0    6    1    0 1015    3    7]\n",
      " [   2    2    0   12    4    7    4    1  995   12]\n",
      " [   4    3    7   12    8    4    0   14    5  930]]\n"
     ]
    }
   ],
   "source": [
    "# Predict using the best model on X_test and y_test\n",
    "y_pred_rf = rf_halving.predict(X_test)\n",
    "print('Accuracy score for Random Forest: ', accuracy_score(y_test, y_pred_rf))\n",
    "print('F1 weighted score for Random Forest: ', f1_score(y_test, y_pred_rf, average='weighted'))\n",
    "print('Confusion matrix for Random Forest: \\n', confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees classifier using HalvingGridSearchCV with param grid\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create ExtraTreesClassifier\n",
    "et = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "# Create param grid\n",
    "param_grid_rt = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'max_features': [0.5, 0.6, 0.7]\n",
    "}\n",
    "\n",
    "# Create HalvingGridSearchCV\n",
    "halving_cv = HalvingGridSearchCV(et, param_grid_rt, cv=2, random_state=42)\n",
    "halving_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print best params\n",
    "print(f'Best params: {halving_cv.best_params_}')\n",
    "print(f'Best score: {halving_cv.best_score_}')\n",
    "\n",
    "# Clasification report\n",
    "y_pred = halving_cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# SVM classifier on MNIST dataset with HalvingGridSearchCV\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = {\n",
    "    'kernel': ['poly'],\n",
    "    'degree': [2, 3, 4],\n",
    "    'C': [1, 10, 100],\n",
    "    'class_weight': ['balanced'],\n",
    "    'coef0': [0.1]}\n",
    "\n",
    "svm_clf = SVC(random_state=42)\n",
    "\n",
    "halving_svc = HalvingGridSearchCV(svm_clf, param_grid, scoring='accuracy', factor=3, random_state=42)\n",
    "halving_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’s class. Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble! Now let’s evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s predictions. How does it compare to the voting classifier you trained earlier?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
