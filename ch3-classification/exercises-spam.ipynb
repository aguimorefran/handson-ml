{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam classificator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem definition\n",
    "\n",
    "We need to classify either if an email is SPAM or not.\n",
    "This is a BINARY CLASSIFICATION problem, where the label will be spam and get binary values 0/1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precission vs Recall tradeoff\n",
    "\n",
    "We also want to study the tradeoff between PRECISION and RECALL.\n",
    "\n",
    "**Precision**: rate of correct classifications. If from 100 mails we classify 70 as spam and 50 are spam, the precision is 50/70. It is, from all the classifications we made, how many of them are correct.\n",
    "\n",
    "**Recall**: rate of detected spams. If from 100 mails there are 80 spams, and we classify correctly 70, the recall is 70/80. It is how many of the mails that are true spams are correctly classified.\n",
    "\n",
    "*Precision* = TP / TP+FP\n",
    "\n",
    "*Recall* = TP / TP+FN\n",
    "\n",
    "So for a spam filter, we want to maximize the recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "DOWNLOAD_PATH = \"datasets\"\n",
    "\n",
    "SPAM_URL = 'https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2'\n",
    "SPAM_2_URL = 'https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2'\n",
    "HAM_URL = 'https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2'\n",
    "\n",
    "def fetch_spam_data(spam_url=SPAM_URL, spam2_url=SPAM_2_URL, ham_url=HAM_URL, download_path=DOWNLOAD_PATH):\n",
    "    if not os.path.isdir(download_path):\n",
    "        os.makedirs(download_path)\n",
    "    for url in (spam_url, spam2_url, ham_url):\n",
    "        filename = url.split('/')[-1]\n",
    "        path = os.path.join(download_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        with tarfile.open(path) as tar:\n",
    "            tar.extractall(path=download_path)\n",
    "\n",
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import pandas as pd\n",
    "\n",
    "SPAM_FOLDER = os.path.join(DOWNLOAD_PATH, 'spam')\n",
    "SPAM2_FOLDER = os.path.join(DOWNLOAD_PATH, 'spam_2')\n",
    "HAM_FOLDER = os.path.join(DOWNLOAD_PATH, 'easy_ham')\n",
    "\n",
    "exclude = ['0000.7b1b73cf36cf9dbc3d64e3f2ee2b91f1']\n",
    "\n",
    "def load_spam_data(spam_folder=SPAM_FOLDER,  spam2_folder=SPAM2_FOLDER, ham_folder=HAM_FOLDER):\n",
    "    data = []\n",
    "    for filename in listdir(spam_folder):\n",
    "        if filename in exclude:\n",
    "            continue\n",
    "        with open(os.path.join(spam_folder, filename), 'rb') as f:\n",
    "            text = f.read().decode('latin-1')\n",
    "            data.append((text, 1))\n",
    "    for filename in listdir(spam2_folder):\n",
    "        with open(os.path.join(spam2_folder, filename), 'rb') as f:\n",
    "            text = f.read().decode('latin-1')\n",
    "            data.append((text, 1))\n",
    "    for filename in listdir(ham_folder):\n",
    "        with open(os.path.join(ham_folder, filename), 'rb') as f:\n",
    "            text = f.read().decode('latin-1')\n",
    "            data.append((text, 0))       \n",
    "\n",
    "    spam_df = pd.DataFrame(data, columns=['text', 'spam'])\n",
    "    return spam_df\n",
    "\n",
    "spam_df = load_spam_data()\n",
    "# save in csv\n",
    "spam_df.to_csv(os.path.join(DOWNLOAD_PATH, 'spam.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From 12a1mailbot1@web.de  Thu Aug 22 13:17:22 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From ilug-admin@linux.ie  Thu Aug 22 13:27:39 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From sabrina@mx3.1premio.com  Thu Aug 22 14:44...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From wsup@playful.com  Thu Aug 22 16:17:00 200...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From social-admin@linux.ie  Thu Aug 22 16:37:3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  From 12a1mailbot1@web.de  Thu Aug 22 13:17:22 ...     1\n",
       "1  From ilug-admin@linux.ie  Thu Aug 22 13:27:39 ...     1\n",
       "2  From sabrina@mx3.1premio.com  Thu Aug 22 14:44...     1\n",
       "3  From wsup@playful.com  Thu Aug 22 16:17:00 200...     1\n",
       "4  From social-admin@linux.ie  Thu Aug 22 16:37:3...     1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n",
      "0    2551\n",
      "1    1897\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(spam_df['spam'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3558\n",
      "Test set size: 890\n",
      "Training set spam ratio: spam\n",
      "0    0.573637\n",
      "1    0.426363\n",
      "Name: count, dtype: float64\n",
      "Test set spam ratio: spam\n",
      "0    0.573034\n",
      "1    0.426966\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(spam_df, spam_df['spam']):\n",
    "    strat_train_set = spam_df.iloc[train_index]\n",
    "    strat_test_set = spam_df.iloc[test_index]\n",
    "\n",
    "print(f\"Training set size: {len(strat_train_set)}\")\n",
    "print(f\"Test set size: {len(strat_test_set)}\")\n",
    "print(f\"Training set spam ratio: {strat_train_set['spam'].value_counts() / len(strat_train_set)}\")\n",
    "print(f\"Test set spam ratio: {strat_test_set['spam'].value_counts() / len(strat_test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to create two types of new features:\n",
    "- stats about the email: proportion of upper/lower chars, num of exclamations, question marks, etc\n",
    "- vector of processed words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mail stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calc stats from sender\n",
    "import re\n",
    "def sender_stats(text):\n",
    "    sender = re.findall(r'From: (.*)', text)[0]\n",
    "    if sender == '':\n",
    "        return np.array([0, 0, 0])\n",
    "    sender_num_rate = len([c for c in sender if c.isdigit()]) / len(sender)\n",
    "    sender_upper_rate = len([c for c in sender if c.isupper()]) / len(sender)\n",
    "    sender_exclamation_rate = len([c for c in sender if c == '!']) / len(sender)\n",
    "\n",
    "    return np.array([sender_num_rate, sender_upper_rate, sender_exclamation_rate])\n",
    "\n",
    "# Function to calc stats from subject\n",
    "\n",
    "CURRENCY_SYMBOLS = ['$', '£', '€', '¥', '₹', '₽', '₩', '₴', '₱', '₲', '₪', '₫', '₵', '₭', '₦', '₸', '₼', '₡', '₢', '₯', '₠', '₧', '₣', '₤', '₶', '₸', '₺', '₼', '₽', '₾', '₿']\n",
    "\n",
    "def subject_stats(text):\n",
    "    subject = re.findall(r'Subject: (.*)', text)[0]\n",
    "    if subject == '':\n",
    "        return np.array([0, 0, 0, 0])\n",
    "    subject_num_rate = len([c for c in subject if c.isdigit()]) / len(subject)\n",
    "    subject_upper_rate = len([c for c in subject if c.isupper()]) / len(subject)\n",
    "    subject_currency_rate = len([c for c in subject if c in CURRENCY_SYMBOLS]) / len(subject)\n",
    "    subject_exclamation_rate = len([c for c in subject if c == '!']) / len(subject)\n",
    "    \n",
    "    return np.array([subject_num_rate, subject_upper_rate, subject_currency_rate, subject_exclamation_rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom Transformer that will be called from a ColumnTransformer\n",
    "# This custom transformer will ingest a Pandas Df of dimmension (n, 1) and return a numpy array of dimmension (n, x), x = number of new features\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class AttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # X is a Pandas Df of dimmension (n, 1)\n",
    "        # Each new feature is a np array of dimmension (n, 1)\n",
    "\n",
    "        # If len in words is 0, return np array of zeros\n",
    "        avg_word_len = X['text'].apply(lambda x: np.divide(len(x), len(x.split()), out=np.zeros_like(float(len(x))), where=len(x.split())!=0)).values.reshape(-1, 1)\n",
    "        rate_upper = X['text'].apply(lambda x: np.divide(len([c for c in x if c.isupper()]), len(x), out=np.zeros_like(float(len(x))), where=len(x)!=0)).values.reshape(-1, 1)\n",
    "        rate_exclamation = X['text'].apply(lambda x: np.divide(len([c for c in x if c == '!']), len(x), out=np.zeros_like(float(len(x))), where=len(x)!=0)).values.reshape(-1, 1)\n",
    "        rate_question = X['text'].apply(lambda x: np.divide(len([c for c in x if c == '?']), len(x), out=np.zeros_like(float(len(x))), where=len(x)!=0)).values.reshape(-1, 1)\n",
    "        sender_num_rate, sender_upper_rate, sender_exclamation_rate = np.array(list(X['text'].apply(sender_stats))).T\n",
    "        subject_num_rate, subject_upper_rate, subject_currency_rate, subject_exclamation_rate = np.array(list(X['text'].apply(subject_stats))).T\n",
    "\n",
    "\n",
    "        return np.c_[\n",
    "            avg_word_len, rate_upper, rate_exclamation, rate_question, \n",
    "            sender_num_rate, sender_upper_rate, sender_exclamation_rate,\n",
    "            subject_num_rate, subject_upper_rate, subject_currency_rate, subject_exclamation_rate\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stats_preprocessing_pipeline = Pipeline([\n",
    "    ('add_stats', AttributesAdder()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create column transformer that will apply the stats_preprocessing_pipeline to the text column\n",
    "stats_column_transformer = ColumnTransformer([\n",
    "    ('stats', stats_preprocessing_pipeline, ['text'])\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  spam\n",
      "113   From thisisagreatfreepornmovie@framesetup.com ...     1\n",
      "1746  From eighbor2k@hotmail.com  Tue Aug  6 10:56:5...     1\n",
      "4253  From rssfeeds@jmason.org  Fri Oct  4 11:02:01 ...     0\n",
      "2981  From exmh-workers-admin@redhat.com  Wed Sep 11...     0\n",
      "2617  From fork-admin@xent.com  Fri Sep 20 11:32:42 ...     0\n",
      "[[ 1.4744794  -1.88372521 -0.64264869 -1.14362847 -0.5        -0.561505\n",
      "   0.         -0.13054181 -1.21175003  0.          0.          1.        ]\n",
      " [-0.1889133   0.33962339  1.93929861  0.11491267  2.         -0.90933209\n",
      "   0.          1.96499771 -1.21175003  0.          0.          1.        ]\n",
      " [-0.6542118  -0.06082571 -0.64264869 -1.14362847 -0.5        -0.90933209\n",
      "   0.         -0.6114853   0.72809218  0.          0.          0.        ]\n",
      " [ 0.72425675  0.65306607 -0.01135254  1.01039893 -0.5         0.87517209\n",
      "   0.         -0.6114853   0.62861309  0.          0.          0.        ]\n",
      " [-1.35561106  0.95186145 -0.64264869  1.16194533 -0.5         1.50499709\n",
      "   0.         -0.6114853   1.06679478  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Test on sample data\n",
    "sample = strat_train_set.sample(5)\n",
    "print(sample)\n",
    "\n",
    "sample_pr = stats_column_transformer.fit_transform(sample)\n",
    "print(sample_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def spamEmailPreprocessor(\n",
    "    email_text,\n",
    "    stop_words_set,\n",
    "    remove_html_header=True,\n",
    "    remove_punctuation=True,\n",
    "    stem=True\n",
    "):\n",
    "    currency_symbols = ['$', '£', '€', '¥', '₹', '₽', '₩', '₴', '₱', '₲', '₪', '₫', '₵', '₭', '₦', '₸', '₼', '₡', '₢', '₯', '₠', '₧', '₣', '₤', '₶', '₸', '₺', '₼', '₽', '₾', '₿']\n",
    "    punctuation_symbols = ['!', '\"', '#', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', \n",
    "                   '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "    if remove_html_header:\n",
    "        # HTML header is separated from the rest of the email by at least one blank line\n",
    "        split_email = email_text.split('\\n\\n', 1)\n",
    "        email_text = split_email[1] if len(split_email) > 1 else split_email[0]\n",
    "\n",
    "    email_text = email_text.lower()\n",
    "    email_text = ' '.join([word.lower() for word in email_text.split() if word.lower() not in stop_words_set])\n",
    "\n",
    "    if remove_punctuation:\n",
    "        for symbol in punctuation_symbols:\n",
    "            email_text = email_text.replace(symbol, ' ')\n",
    "\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        email_text = ' '.join([stemmer.stem(word) for word in email_text.split()])\n",
    "\n",
    "    # Replace numbers with NUMBER xx or xx.xx\n",
    "    email_text = re.sub(r'\\d+\\.\\d+', ' NUMBER ', email_text)\n",
    "    email_text = re.sub(r'\\d+', ' NUMBER ', email_text)\n",
    "\n",
    "    # Replace currency symbols with CURRENCY\n",
    "    for symbol in currency_symbols:\n",
    "        email_text = email_text.replace(symbol, ' CURRENCY ')\n",
    "\n",
    "    # Replace email addresses with EMAIL\n",
    "    email_text = re.sub(r'\\S+@\\S+', ' EMAIL ', email_text)\n",
    "\n",
    "    # Replace URLs with URL\n",
    "    email_text = re.sub(r'http\\S+', ' URL ', email_text)\n",
    "\n",
    "    # Replace IP addresses with IP\n",
    "    email_text = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', ' IP ', email_text)\n",
    "    \n",
    "    return email_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words_en = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got bad credit fix it ye fix credit report easi use softwar attorney fee simpli download way good credit get rid neg credit report easili nucredit avail two version nucredit individu use  CURRENCY  NUMBER   NUMBER  credit pro use  NUMBER  peopl  CURRENCY  NUMBER   NUMBER  follow link start repair credit today http  NUMBER   NUMBER   NUMBER   NUMBER  inbox NUMBER  wish remov mail list pleas click link indic address receiv origin email properli remov list mailto nucredit NUMBER  yahoo com subject remov\n"
     ]
    }
   ],
   "source": [
    "# Test on rand email\n",
    "rand_email = strat_train_set.sample(1, random_state=41)['text'].values[0]\n",
    "# Preprocess\n",
    "rand_email_pr = spamEmailPreprocessor(rand_email, stop_words_en)\n",
    "print(rand_email_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom Transformer, which Transform will apply the spamEmailPreprocessor function to the text column\n",
    "\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "class EmailPreprocessorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, remove_html_header=True, remove_punctuation=True, stem=True):\n",
    "        self.stop_words_set = stop_words_set\n",
    "        self.remove_html_header = remove_html_header\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.stem = stem\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_pr = X['text'].apply(\n",
    "            lambda x: spamEmailPreprocessor(\n",
    "                x, \n",
    "                self.stop_words_set, \n",
    "                self.remove_html_header, \n",
    "                self.remove_punctuation, \n",
    "                self.stem\n",
    "            )\n",
    "        ).values.reshape(-1, 1)\n",
    "\n",
    "        return X_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer custom transformer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class VectorizerTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_features=1000):\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.vectorizer = CountVectorizer(max_features=self.max_features)\n",
    "        self.vectorizer.fit(X.ravel())\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.vectorizer.transform(X.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = EmailPreprocessorTransformer()\n",
    "\n",
    "text_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('vectorizer', VectorizerTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 400)\n"
     ]
    }
   ],
   "source": [
    "# Final pipeline\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('stats', stats_preprocessing_pipeline, ['text']),\n",
    "    ('text', text_pipeline, ['text'])\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Test on sample data\n",
    "sample = strat_train_set.sample(5)\n",
    "# print(sample)\n",
    "\n",
    "sample_pr = full_pipeline.fit_transform(sample)\n",
    "print(sample_pr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4448, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of samples\n",
    "spam_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVC -> Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m strat_train_set_pr \u001b[39m=\u001b[39m full_pipeline\u001b[39m.\u001b[39;49mfit_transform(strat_train_set)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:743\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_column_callables(X)\n\u001b[0;32m    741\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m--> 743\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(X, y, _fit_transform_one)\n\u001b[0;32m    745\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result:\n\u001b[0;32m    746\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:670\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[1;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[0;32m    664\u001b[0m transformers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m    665\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(\n\u001b[0;32m    666\u001b[0m         fitted\u001b[39m=\u001b[39mfitted, replace_strings\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, column_as_strings\u001b[39m=\u001b[39mcolumn_as_strings\n\u001b[0;32m    667\u001b[0m     )\n\u001b[0;32m    668\u001b[0m )\n\u001b[0;32m    669\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 670\u001b[0m     \u001b[39mreturn\u001b[39;00m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[0;32m    671\u001b[0m         delayed(func)(\n\u001b[0;32m    672\u001b[0m             transformer\u001b[39m=\u001b[39;49mclone(trans) \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m fitted \u001b[39melse\u001b[39;49;00m trans,\n\u001b[0;32m    673\u001b[0m             X\u001b[39m=\u001b[39;49m_safe_indexing(X, column, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[0;32m    674\u001b[0m             y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    675\u001b[0m             weight\u001b[39m=\u001b[39;49mweight,\n\u001b[0;32m    676\u001b[0m             message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mColumnTransformer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    677\u001b[0m             message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(name, idx, \u001b[39mlen\u001b[39;49m(transformers)),\n\u001b[0;32m    678\u001b[0m         )\n\u001b[0;32m    679\u001b[0m         \u001b[39mfor\u001b[39;49;00m idx, (name, trans, column, weight) \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(transformers, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    680\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    682\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\joblib\\parallel.py:1855\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1853\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1854\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1855\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1857\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1858\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1859\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1862\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\joblib\\parallel.py:1784\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1783\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1784\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1785\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1786\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    949\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 950\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit_transform(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    951\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\pipeline.py:464\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \n\u001b[0;32m    439\u001b[0m \u001b[39mFits all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39m    Transformed samples.\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    463\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 464\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[0;32m    466\u001b[0m last_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\n\u001b[0;32m    467\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\pipeline.py:370\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    368\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    369\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    371\u001b[0m     cloned_transformer,\n\u001b[0;32m    372\u001b[0m     X,\n\u001b[0;32m    373\u001b[0m     y,\n\u001b[0;32m    374\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    375\u001b[0m     message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    376\u001b[0m     message\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    377\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    949\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 950\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit_transform(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    951\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[0;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "Cell \u001b[1;32mIn[55], line 16\u001b[0m, in \u001b[0;36mEmailPreprocessorTransformer.transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 16\u001b[0m     X_pr \u001b[39m=\u001b[39m X[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\n\u001b[0;32m     17\u001b[0m         \u001b[39mlambda\u001b[39;49;00m x: spamEmailPreprocessor(\n\u001b[0;32m     18\u001b[0m             x, \n\u001b[0;32m     19\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstop_words_set, \n\u001b[0;32m     20\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mremove_html_header, \n\u001b[0;32m     21\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mremove_punctuation, \n\u001b[0;32m     22\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstem\n\u001b[0;32m     23\u001b[0m         )\n\u001b[0;32m     24\u001b[0m     )\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m X_pr\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[55], line 17\u001b[0m, in \u001b[0;36mEmailPreprocessorTransformer.transform.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     16\u001b[0m     X_pr \u001b[39m=\u001b[39m X[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\n\u001b[1;32m---> 17\u001b[0m         \u001b[39mlambda\u001b[39;00m x: spamEmailPreprocessor(\n\u001b[0;32m     18\u001b[0m             x, \n\u001b[0;32m     19\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstop_words_set, \n\u001b[0;32m     20\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mremove_html_header, \n\u001b[0;32m     21\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mremove_punctuation, \n\u001b[0;32m     22\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstem\n\u001b[0;32m     23\u001b[0m         )\n\u001b[0;32m     24\u001b[0m     )\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m X_pr\n",
      "Cell \u001b[1;32mIn[52], line 29\u001b[0m, in \u001b[0;36mspamEmailPreprocessor\u001b[1;34m(email_text, stop_words_set, remove_html_header, remove_punctuation, stem)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mif\u001b[39;00m stem:\n\u001b[0;32m     28\u001b[0m     stemmer \u001b[39m=\u001b[39m PorterStemmer()\n\u001b[1;32m---> 29\u001b[0m     email_text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([stemmer\u001b[39m.\u001b[39mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m email_text\u001b[39m.\u001b[39msplit()])\n\u001b[0;32m     31\u001b[0m \u001b[39m# Replace numbers with NUMBER xx or xx.xx\u001b[39;00m\n\u001b[0;32m     32\u001b[0m email_text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m NUMBER \u001b[39m\u001b[39m'\u001b[39m, email_text)\n",
      "Cell \u001b[1;32mIn[52], line 29\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mif\u001b[39;00m stem:\n\u001b[0;32m     28\u001b[0m     stemmer \u001b[39m=\u001b[39m PorterStemmer()\n\u001b[1;32m---> 29\u001b[0m     email_text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([stemmer\u001b[39m.\u001b[39;49mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m email_text\u001b[39m.\u001b[39msplit()])\n\u001b[0;32m     31\u001b[0m \u001b[39m# Replace numbers with NUMBER xx or xx.xx\u001b[39;00m\n\u001b[0;32m     32\u001b[0m email_text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m NUMBER \u001b[39m\u001b[39m'\u001b[39m, email_text)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\nltk\\stem\\porter.py:674\u001b[0m, in \u001b[0;36mPorterStemmer.stem\u001b[1;34m(self, word, to_lowercase)\u001b[0m\n\u001b[0;32m    672\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step2(stem)\n\u001b[0;32m    673\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step3(stem)\n\u001b[1;32m--> 674\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step4(stem)\n\u001b[0;32m    675\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step5a(stem)\n\u001b[0;32m    676\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step5b(stem)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\nltk\\stem\\porter.py:573\u001b[0m, in \u001b[0;36mPorterStemmer._step4\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Implements Step 4 from \"An algorithm for suffix stripping\"\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \n\u001b[0;32m    546\u001b[0m \u001b[39mStep 4\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mtidying up.\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    571\u001b[0m measure_gt_1 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m stem: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_measure(stem) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 573\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_rule_list(\n\u001b[0;32m    574\u001b[0m     word,\n\u001b[0;32m    575\u001b[0m     [\n\u001b[0;32m    576\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mal\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    577\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mance\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    578\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mence\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    579\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mer\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    580\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mic\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    581\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mable\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    582\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mible\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    583\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mant\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    584\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mement\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    585\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mment\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    586\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39ment\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    587\u001b[0m         \u001b[39m# (m>1 and (*S or *T)) ION ->\u001b[39;49;00m\n\u001b[0;32m    588\u001b[0m         (\n\u001b[0;32m    589\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mion\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    590\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    591\u001b[0m             \u001b[39mlambda\u001b[39;49;00m stem: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_measure(stem) \u001b[39m>\u001b[39;49m \u001b[39m1\u001b[39;49m \u001b[39mand\u001b[39;49;00m stem[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39min\u001b[39;49;00m (\u001b[39m\"\u001b[39;49m\u001b[39ms\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mt\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    592\u001b[0m         ),\n\u001b[0;32m    593\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mou\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    594\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mism\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    595\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mate\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    596\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39miti\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    597\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mous\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    598\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mive\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    599\u001b[0m         (\u001b[39m\"\u001b[39;49m\u001b[39mize\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, measure_gt_1),\n\u001b[0;32m    600\u001b[0m     ],\n\u001b[0;32m    601\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\nltk\\stem\\porter.py:268\u001b[0m, in \u001b[0;36mPorterStemmer._apply_rule_list\u001b[1;34m(self, word, rules)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mif\u001b[39;00m word\u001b[39m.\u001b[39mendswith(suffix):\n\u001b[0;32m    267\u001b[0m     stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replace_suffix(word, suffix, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 268\u001b[0m     \u001b[39mif\u001b[39;00m condition \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m condition(stem):\n\u001b[0;32m    269\u001b[0m         \u001b[39mreturn\u001b[39;00m stem \u001b[39m+\u001b[39m replacement\n\u001b[0;32m    270\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         \u001b[39m# Don't try any further rules\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\nltk\\stem\\porter.py:571\u001b[0m, in \u001b[0;36mPorterStemmer._step4.<locals>.<lambda>\u001b[1;34m(stem)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_step4\u001b[39m(\u001b[39mself\u001b[39m, word):\n\u001b[0;32m    544\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Implements Step 4 from \"An algorithm for suffix stripping\"\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \n\u001b[0;32m    546\u001b[0m \u001b[39m    Step 4\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39m    tidying up.\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 571\u001b[0m     measure_gt_1 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m stem: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_measure(stem) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    573\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_rule_list(\n\u001b[0;32m    574\u001b[0m         word,\n\u001b[0;32m    575\u001b[0m         [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m         ],\n\u001b[0;32m    601\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\nltk\\stem\\porter.py:191\u001b[0m, in \u001b[0;36mPorterStemmer._measure\u001b[1;34m(self, stem)\u001b[0m\n\u001b[0;32m    189\u001b[0m         cv_sequence \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mc\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m         cv_sequence \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[39m# Count the number of 'vc' occurrences, which is equivalent to\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m# the number of 'VC' occurrences in Porter's reduced form in the\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# docstring above, which is in turn equivalent to `m`\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mreturn\u001b[39;00m cv_sequence\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39mvc\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "strat_train_set_pr = full_pipeline.fit_transform(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3558, 1011)\n",
      "(3558, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_pr = strat_train_set_pr[:, :-1]\n",
    "print(X_train_pr.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 400 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m svc_clf \u001b[39m=\u001b[39m SVC()\n\u001b[0;32m     25\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(pipeline_with_svc, param_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(strat_train_set, strat_train_set[\u001b[39m'\u001b[39;49m\u001b[39mspam\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\joblib\\parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1938\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1939\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1941\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1942\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1944\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\joblib\\parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1584\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1586\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1587\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1589\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1590\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1591\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\faguilera\\miniconda3\\envs\\ml\\lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1698\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1699\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   1700\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SVC Classifier with GridSearchCV\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Those are hyperparameters that will be tuned for the SVC\n",
    "\n",
    "pipeline_with_svc = Pipeline([\n",
    "    ('preprocessing', full_pipeline),\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'preprocessing__text__preprocessor__remove_html_header': [True, False], \n",
    "        'preprocessing__text__preprocessor__remove_punctuation': [True, False], \n",
    "        'preprocessing__text__preprocessor__stem': [True, False], \n",
    "        'preprocessing__text__vectorizer__max_features': [1000, 2000, 3000, 4000, 5000], \n",
    "        'svc__kernel': ['linear', 'rbf'], \n",
    "        'svc__C': [0.1, 1, 10, 100, 1000]\n",
    "    }\n",
    "]\n",
    "\n",
    "svc_clf = SVC()\n",
    "\n",
    "grid_search = GridSearchCV(pipeline_with_svc, param_grid, cv=5, scoring='accuracy', verbose=10, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(strat_train_set_pr, strat_train_set['spam'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
