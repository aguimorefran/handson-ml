{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam classificator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem definition\n",
    "\n",
    "We need to classify either if an email is SPAM or not.\n",
    "This is a BINARY CLASSIFICATION problem, where the label will be spam and get binary values 0/1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precission vs Recall tradeoff\n",
    "\n",
    "We also want to study the tradeoff between PRECISION and RECALL.\n",
    "\n",
    "**Precision**: rate of correct classifications. If from 100 mails we classify 70 as spam and 50 are spam, the precision is 50/70. It is, from all the classifications we made, how many of them are correct.\n",
    "\n",
    "**Recall**: rate of detected spams. If from 100 mails there are 80 spams, and we classify correctly 70, the recall is 70/80. It is how many of the mails that are true spams are correctly classified.\n",
    "\n",
    "*Precision* = TP / TP+FP\n",
    "\n",
    "*Recall* = TP / TP+FN\n",
    "\n",
    "So for a spam filter, we want to maximize the recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_PATH = \"datasets\"\n",
    "\n",
    "SPAM_URL = 'https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2'\n",
    "SPAM_2_URL = 'https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2'\n",
    "HAM_URL = 'https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2'\n",
    "\n",
    "def fetch_spam_data(spam_url=SPAM_URL, spam2_url=SPAM_2_URL, ham_url=HAM_URL, download_path=DOWNLOAD_PATH):\n",
    "    if not os.path.isdir(download_path):\n",
    "        os.makedirs(download_path)\n",
    "    for url in (spam_url, spam2_url, ham_url):\n",
    "        filename = url.split('/')[-1]\n",
    "        path = os.path.join(download_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        with tarfile.open(path) as tar:\n",
    "            tar.extractall(path=download_path)\n",
    "\n",
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import pandas as pd\n",
    "\n",
    "SPAM_FOLDER = os.path.join(DOWNLOAD_PATH, 'spam')\n",
    "SPAM2_FOLDER = os.path.join(DOWNLOAD_PATH, 'spam_2')\n",
    "HAM_FOLDER = os.path.join(DOWNLOAD_PATH, 'easy_ham')\n",
    "\n",
    "exclude = ['0000.7b1b73cf36cf9dbc3d64e3f2ee2b91f1']\n",
    "\n",
    "def load_spam_data(spam_folder=SPAM_FOLDER,  spam2_folder=SPAM2_FOLDER, ham_folder=HAM_FOLDER):\n",
    "    data = []\n",
    "    for filename in listdir(spam_folder):\n",
    "        if filename in exclude:\n",
    "            continue\n",
    "        with open(os.path.join(spam_folder, filename), 'rb') as f:\n",
    "            text = f.read().decode('latin-1')\n",
    "            data.append((text, 1))\n",
    "    for filename in listdir(spam2_folder):\n",
    "        with open(os.path.join(spam2_folder, filename), 'rb') as f:\n",
    "            text = f.read().decode('latin-1')\n",
    "            data.append((text, 1))\n",
    "    for filename in listdir(ham_folder):\n",
    "        with open(os.path.join(ham_folder, filename), 'rb') as f:\n",
    "            text = f.read().decode('latin-1')\n",
    "            data.append((text, 0))       \n",
    "\n",
    "    spam_df = pd.DataFrame(data, columns=['text', 'spam'])\n",
    "    return spam_df\n",
    "\n",
    "spam_df = load_spam_data()\n",
    "# save in csv\n",
    "spam_df.to_csv(os.path.join(DOWNLOAD_PATH, 'spam.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From 12a1mailbot1@web.de  Thu Aug 22 13:17:22 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From ilug-admin@linux.ie  Thu Aug 22 13:27:39 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From sabrina@mx3.1premio.com  Thu Aug 22 14:44...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From wsup@playful.com  Thu Aug 22 16:17:00 200...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From social-admin@linux.ie  Thu Aug 22 16:37:3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  From 12a1mailbot1@web.de  Thu Aug 22 13:17:22 ...     1\n",
       "1  From ilug-admin@linux.ie  Thu Aug 22 13:27:39 ...     1\n",
       "2  From sabrina@mx3.1premio.com  Thu Aug 22 14:44...     1\n",
       "3  From wsup@playful.com  Thu Aug 22 16:17:00 200...     1\n",
       "4  From social-admin@linux.ie  Thu Aug 22 16:37:3...     1"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n",
      "0    2551\n",
      "1    1897\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(spam_df['spam'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3558\n",
      "Test set size: 890\n",
      "Training set spam ratio: spam\n",
      "0    0.573637\n",
      "1    0.426363\n",
      "Name: count, dtype: float64\n",
      "Test set spam ratio: spam\n",
      "0    0.573034\n",
      "1    0.426966\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(spam_df, spam_df['spam']):\n",
    "    strat_train_set = spam_df.iloc[train_index]\n",
    "    strat_test_set = spam_df.iloc[test_index]\n",
    "\n",
    "print(f\"Training set size: {len(strat_train_set)}\")\n",
    "print(f\"Test set size: {len(strat_test_set)}\")\n",
    "print(f\"Training set spam ratio: {strat_train_set['spam'].value_counts() / len(strat_train_set)}\")\n",
    "print(f\"Test set spam ratio: {strat_test_set['spam'].value_counts() / len(strat_test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to create two types of new features:\n",
    "- stats about the email: proportion of upper/lower chars, num of exclamations, question marks, etc\n",
    "- vector of processed words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mail stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calc stats from sender\n",
    "import re\n",
    "def sender_stats(text):\n",
    "    sender = re.findall(r'From: (.*)', text)[0]\n",
    "    sender_num_rate = len([c for c in sender if c.isdigit()]) / len(sender)\n",
    "    sender_upper_rate = len([c for c in sender if c.isupper()]) / len(sender)\n",
    "    sender_exclamation_rate = len([c for c in sender if c == '!']) / len(sender)\n",
    "\n",
    "    return np.array([sender_num_rate, sender_upper_rate, sender_exclamation_rate])\n",
    "\n",
    "# Function to calc stats from subject\n",
    "\n",
    "CURRENCY_SYMBOLS = ['$', '£', '€', '¥', '₹', '₽', '₩', '₴', '₱', '₲', '₪', '₫', '₵', '₭', '₦', '₸', '₼', '₡', '₢', '₯', '₠', '₧', '₣', '₤', '₶', '₸', '₺', '₼', '₽', '₾', '₿']\n",
    "\n",
    "def subject_stats(text):\n",
    "    subject = re.findall(r'Subject: (.*)', text)[0]\n",
    "    subject_num_rate = len([c for c in subject if c.isdigit()]) / len(subject)\n",
    "    subject_upper_rate = len([c for c in subject if c.isupper()]) / len(subject)\n",
    "    subject_currency_rate = len([c for c in subject if c in CURRENCY_SYMBOLS]) / len(subject)\n",
    "    subject_exclamation_rate = len([c for c in subject if c == '!']) / len(subject)\n",
    "    \n",
    "    return np.array([subject_num_rate, subject_upper_rate, subject_currency_rate, subject_exclamation_rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom Transformer that will be called from a ColumnTransformer\n",
    "# This custom transformer will ingest a Pandas Df of dimmension (n, 1) and return a numpy array of dimmension (n, x), x = number of new features\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        # X is a Pandas Df of dimmension (n, 1)\n",
    "        # Each new feature is a np array of dimmension (n, 1)\n",
    "\n",
    "        avg_word_len = X['text'].apply(lambda x: len(x) / len(x.split())).values.reshape(-1, 1)\n",
    "        rate_upper = X['text'].apply(lambda x: len([c for c in x if c.isupper()]) / len(x)).values.reshape(-1, 1)\n",
    "        rate_exclamation = X['text'].apply(lambda x: len([c for c in x if c == '!']) / len(x)).values.reshape(-1, 1)\n",
    "        rate_question = X['text'].apply(lambda x: len([c for c in x if c == '?']) / len(x)).values.reshape(-1, 1)\n",
    "        # Sender stats\n",
    "        sender_num_rate, sender_upper_rate, sender_exclamation_rate = np.array(list(X['text'].apply(sender_stats))).T\n",
    "        # Subject stats\n",
    "        subject_num_rate, subject_upper_rate, subject_currency_rate, subject_exclamation_rate = np.array(list(X['text'].apply(subject_stats))).T\n",
    "\n",
    "\n",
    "        return np.c_[\n",
    "            avg_word_len, rate_upper, rate_exclamation, rate_question, \n",
    "            sender_num_rate, sender_upper_rate, sender_exclamation_rate,\n",
    "            subject_num_rate, subject_upper_rate, subject_currency_rate, subject_exclamation_rate\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that adds stats features and then uses StdScaler to scale the data\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stats_preprocessing_pipeline = Pipeline([\n",
    "    ('add_stats', AttributesAdder()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create column transformer that will apply the stats_preprocessing_pipeline to the text column\n",
    "stats_column_transformer = ColumnTransformer([\n",
    "    ('stats', stats_preprocessing_pipeline, ['text'])\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  spam\n",
      "104   From hadley@cb.offermonkey.com  Mon Aug 26 20:...     1\n",
      "347   Return-Path: ler@lerami.lerctr.org\\nDelivery-D...     1\n",
      "1305  From money@viplook.net  Mon Jul 22 18:09:50 20...     1\n",
      "3829  From rssfeeds@jmason.org  Tue Sep 24 10:47:26 ...     0\n",
      "2761  From fork-admin@xent.com  Mon Sep 30 13:53:28 ...     0\n",
      "[[ 0.92059662  1.30098222 -0.5         1.57704399  0.          1.03190896\n",
      "   0.          0.50822246  0.7387665  -0.5         0.          1.        ]\n",
      " [-0.47931231 -0.44061642 -0.5        -0.16764798  0.          0.85953535\n",
      "   0.         -0.95156546 -0.37867197 -0.5         0.          1.        ]\n",
      " [-1.63988276 -0.25936013  2.         -1.0209632   0.          0.01522388\n",
      "   0.          1.68686604 -1.36655236  2.          0.          1.        ]\n",
      " [ 1.10685563 -1.49746375 -0.5        -1.0209632   0.         -1.78352664\n",
      "   0.         -0.95156546 -0.48086649 -0.5         0.          0.        ]\n",
      " [ 0.09174282  0.89645808 -0.5         0.63253039  0.         -0.12314154\n",
      "   0.         -0.29195758  1.48732432 -0.5         0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Test on sample data\n",
    "sample = strat_train_set.sample(5)\n",
    "print(sample)\n",
    "\n",
    "sample_pr = stats_column_transformer.fit_transform(sample)\n",
    "print(sample_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word vectorizer pipeline that will be applied to the text column\n",
    "# It must replace NUM, URL, EMAIL, CURRENCY, IP\n",
    "# It must take hyperparameters: tolower, word_min_len, stem, strip_header\n",
    "# This vectorizer will be called from a ColumnTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "class WordVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, tolower=True, stem=True, strip_header=True):\n",
    "        self.tolower = tolower\n",
    "        self.stem = stem\n",
    "        self.strip_header = strip_header\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # X is a Pandas Df of dimmension (n, 1)\n",
    "        # Each new feature is a np array of dimmension (n, 1)\n",
    "        X = X.copy()\n",
    "        X['text'] = X['text'].apply(self._clean_text)\n",
    "        return X['text'].values\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        if self.strip_header:\n",
    "            # Header is everything before the first blank line\n",
    "            text = text.split('\\n\\n', 1)[1]\n",
    "\n",
    "        if self.tolower:\n",
    "            text = text.lower()\n",
    "\n",
    "        # Remove stop words\n",
    "        text = ' '.join([w for w in text.split() if w not in self.stopwords])\n",
    "\n",
    "        # NUM\n",
    "        text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUM', text)\n",
    "        # URL\n",
    "        text = re.sub(r'(?:https?|ftp)://[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', 'URL', text)\n",
    "        # EMAIL\n",
    "        text = re.sub(r'\\S+@\\S+', 'EMAIL', text)\n",
    "        # CURRENCY\n",
    "        # any char in CURRENCY_SYMBOLS\n",
    "        text = re.sub(r'[{}]+'.format(''.join(CURRENCY_SYMBOLS)), 'CURRENCY', text)\n",
    "        # IP\n",
    "        text = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', 'IP', text)\n",
    "\n",
    "        # STEM\n",
    "        if self.stem:\n",
    "            text = ' '.join([self.stemmer.stem(w) for w in text.split()])\n",
    "        \n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From johnhall@evergo.net  Tue Sep 17 23:29:48 2002\\nReturn-Path: <johnhall@evergo.net>\\nDelivered-To: yyyy@localhost.example.com\\nReceived: from localhost (jalapeno [127.0.0.1])\\n\\tby jmason.org (Postfix) with ESMTP id CF5C316F03\\n\\tfor <jm@localhost>; Tue, 17 Sep 2002 23:29:47 +0100 (IST)\\nReceived: from jalapeno [127.0.0.1]\\n\\tby localhost with IMAP (fetchmail-5.9.0)\\n\\tfor jm@localhost (single-drop); Tue, 17 Sep 2002 23:29:47 +0100 (IST)\\nReceived: from mail.evergo.net ([206.191.151.2]) by dogma.slashnull.org\\n    (8.11.6/8.11.6) with SMTP id g8HKUvC25665 for <jm@jmason.org>;\\n    Tue, 17 Sep 2002 21:30:58 +0100\\nReceived: (qmail 31515 invoked from network); 17 Sep 2002 20:31:20 -0000\\nReceived: from dsl.206.191.151.102.evergo.net (HELO JMHALL)\\n    (206.191.151.102) by mail.evergo.net with SMTP; 17 Sep 2002 20:31:20 -0000\\nReply-To: <johnhall@evergo.net>\\nFrom: \"John Hall\" <johnhall@evergo.net>\\nTo: <yyyy@example.com>, \"\\'Gary Lawrence Murphy\\'\" <garym@canada.com>\\nCc: \"\\'Stephen D. Williams\\'\" <sdw@lig.net>, <fork@example.com>,\\n\\t<lea@lig.net>\\nSubject: RE: Slaughter in the Name of God\\nDate: Tue, 17 Sep 2002 13:31:20 -0700\\nMessage-Id: <001601c25e89$2f06a3d0$0200a8c0@JMHALL>\\nMIME-Version: 1.0\\nContent-Type: text/plain; charset=\"us-ascii\"\\nContent-Transfer-Encoding: 7bit\\nX-Priority: 3 (Normal)\\nX-Msmail-Priority: Normal\\nX-Mailer: Microsoft Outlook, Build 10.0.2627\\nIn-Reply-To: <20020917165028.4F4EA16F03@example.com>\\nImportance: Normal\\nX-Mimeole: Produced By Microsoft MimeOLE V6.00.2600.0000\\nX-Spam-Status: No, hits=-6.0 required=7.0\\n\\ttests=AWL,IN_REP_TO,QUOTED_EMAIL_TEXT\\n\\tversion=2.50-cvs\\nX-Spam-Level: \\n\\n\\n\\n> From: yyyy@example.com [mailto:yyyy@example.com]\\n> Sent: Tuesday, September 17, 2002 9:50 AM\\n> > ... we\\'ve been fighting the War on Terrorism for as long\\n> > as there\\'s been commerce, so you\\'d think we\\'d /realize/ that\\n> > escalation of violence is not a solution.\\n> \\n> Well said!\\n> \\n> --j.\\n\\nYeah.  It certainly wasn\\'t a solution to the Carthaginian problem or the\\nBarbary Pirates.  Wait ... no ... actually ... it was a rather permanent\\nsolution.\\n\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "corpus = strat_train_set['text'].sample(1).values\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"> from: email email > sent: tuesday, septemb num, num num:num > > ... we'v fight war terror long > > there' commerce, think we'd /realize/ > > escal violenc solution. > > well said! > > --j. yeah. certainli solut carthaginian problem barbari pirates. wait ... ... actual ... rather perman solution.\"]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = WordVectorizer()\n",
    "X_vec = vectorizer.fit_transform(pd.DataFrame(corpus, columns=['text']))\n",
    "print(X_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: 0\n",
      "EMAIL: 0\n",
      "CURRENCY: 0\n",
      "IP: 0\n",
      "NUM: 0\n"
     ]
    }
   ],
   "source": [
    "# Count how many URL, EMAIL, CURRENCY, IP, NUM, PHONE, TIME, DATE, PERCENT are in the X_vec\n",
    "num_url = np.sum(X_vec == 'URL')\n",
    "num_email = np.sum(X_vec == 'EMAIL')\n",
    "num_currency = np.sum(X_vec == 'CURRENCY')\n",
    "num_ip = np.sum(X_vec == 'IP')\n",
    "num_num = np.sum(X_vec == 'NUM')\n",
    "\n",
    "print(f\"URL: {num_url}\")\n",
    "print(f\"EMAIL: {num_email}\")\n",
    "print(f\"CURRENCY: {num_currency}\")\n",
    "print(f\"IP: {num_ip}\")\n",
    "print(f\"NUM: {num_num}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
