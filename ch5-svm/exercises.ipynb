{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is the fundamental idea behind Support Vector Machines?\n",
    "\n",
    "It is to create a margin with the max width so the \"separation\" between classes is well differentiated.\n",
    "\n",
    "## 2. What is a support vector?\n",
    "\n",
    "The support vectors are the two perpendicular lines to the margin, one positive and one negative. These support vectors represente the perpendicular distance at which the closest samples to the margin are. Depending on soft or hard margin, samples might appear inside these vectors.\n",
    "\n",
    "## 3. Why is it important to scale the inputs when using SVMs?\n",
    "\n",
    "If the data is not scaled then margins can be narrower or not be possible at all.\n",
    "\n",
    "## 4. Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?\n",
    "\n",
    "No. I think it can only classify to 0 or 1.\n",
    "\n",
    "## 5. Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?\n",
    "\n",
    "As primal is better for problems with trainin instances < number of features, we could say that Dual is better. It also lets us peform kernel trick which can speed up the training process.\n",
    "\n",
    "## 6. Say you trained an SVM classifier with an RBF kernel. It seems to underfit the training set: should you increase or decrease Î³ (gamma)? What about C?\n",
    "\n",
    "It depends on the shape of the margin and the data. If we have a cloud of points and underfitting, we can try to shape the margin around the cloud. First by increasing gamma so the margin closes. Then increasing C so the margin is softer or harder depending on what we want.\n",
    "\n",
    "## 7. How should you set the QP parameters (H, f, A, and b) to solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver?\n",
    "\n",
    "H is a nxn identity matrix, being n the number of samples\n",
    "f = 0s vector of size m + 1, being m the number of features\n",
    "b = 1s vector of size n\n",
    "A = -t * x', being x' equal to x plus bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will need to use one-versus-all to classify all 10 digits. You may want to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you reach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train an SVM regressor on the California housing dataset.    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
